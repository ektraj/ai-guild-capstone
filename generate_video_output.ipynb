{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Test the disaster identifying model on a drone video\n#### Specify path of a test video and output video","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, division\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom collections import deque\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport cv2\nfrom collections import deque\ncudnn.benchmark = True\nplt.ion()   # interactive mode\nfrom PIL import Image\n\nfrom omnixai.data.image import Image\nfrom omnixai.explainers.vision.specific.gradcam.pytorch.gradcam import GradCAM\nimport plotly.io as pio\npio.renderers.default = \"png\"\nfrom IPython.display import display # to display images","metadata":{"execution":{"iopub.status.busy":"2022-12-02T14:32:13.817925Z","iopub.execute_input":"2022-12-02T14:32:13.818281Z","iopub.status.idle":"2022-12-02T14:32:13.826864Z","shell.execute_reply.started":"2022-12-02T14:32:13.818254Z","shell.execute_reply":"2022-12-02T14:32:13.825890Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# The preprocessing model\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    #transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\npreprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims])","metadata":{"execution":{"iopub.status.busy":"2022-12-02T14:27:46.567614Z","iopub.execute_input":"2022-12-02T14:27:46.568319Z","iopub.status.idle":"2022-12-02T14:27:46.574389Z","shell.execute_reply.started":"2022-12-02T14:27:46.568282Z","shell.execute_reply":"2022-12-02T14:27:46.573221Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model = torch.load('trained_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-02T14:27:48.792362Z","iopub.execute_input":"2022-12-02T14:27:48.792736Z","iopub.status.idle":"2022-12-02T14:27:48.850035Z","shell.execute_reply.started":"2022-12-02T14:27:48.792689Z","shell.execute_reply":"2022-12-02T14:27:48.849012Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\ntest_video = '/kaggle/input/test-video/gettyimages-1207220141-640_adpp.mp4'\noutput_video = 'output.mp4'","metadata":{"execution":{"iopub.status.busy":"2022-12-02T14:28:56.532512Z","iopub.execute_input":"2022-12-02T14:28:56.532914Z","iopub.status.idle":"2022-12-02T14:28:56.538282Z","shell.execute_reply.started":"2022-12-02T14:28:56.532879Z","shell.execute_reply":"2022-12-02T14:28:56.536971Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"CLASSES = {0:\"Collapsed Building\", 1:\"Fire\", 2:\"Flood\", 3:\"Normal\"}\nBATCH_SIZE = 8\nIMG_SIZE = (224, 224)\nTRANSFORM_IMG = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(IMG_SIZE),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225] )\n    ])\n\n\npreprocess = lambda ims: torch.stack([TRANSFORM_IMG(im.to_pil()) for im in ims])\nexplainer = GradCAM(\n    model=model,\n    target_layer=model.layer4[-1],\n    preprocess_function=preprocess\n)\n\n\n\n# model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nvideoCapture = cv2.VideoCapture(test_video)\nfps = videoCapture.get(cv2.CAP_PROP_FPS)\nprint(fps)\nsize = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\nfourcc = cv2.VideoWriter_fourcc(*'DIVX')\nvideoWriter = cv2.VideoWriter(output_video, fourcc, fps, size)\nQ = deque(maxlen=int(fps))\nt1 = time.time()\nc=0\nsuccess, frame = videoCapture.read()\nwhile success:\n    c+=1\n    frame_copy = copy.deepcopy(frame) \n    frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n    image_gc = Image(copy.deepcopy(frame_copy))\n\n\n    # Explain the top label\n    explanations = explainer.explain(image_gc)\n    image_tensor = TRANSFORM_IMG(frame_copy)\n\n    image_tensor = image_tensor.unsqueeze(0) \n    test_input = image_tensor.to(device)\n    outputs = model(test_input)\n    _, predicted = torch.max(outputs, 1)\n    probability =  F.softmax(outputs, dim=1)\n    top_probability, top_class = probability.topk(1, dim=1)\n    predicted = predicted.cpu().detach().numpy()\n    predicted = predicted.tolist()[0]\n    Q.append(predicted)\n\n    results = np.array(Q).mean(axis=0)\n    #i = np.argmax(results)\n    #print(Q, results, CLASSES[np.round(results)])\n    label =CLASSES[np.round(results)]\n    top_probability = top_probability.cpu().detach().numpy()\n    top_probability = top_probability.tolist()[0][0]\n    percentage = top_probability\n    top_probability = '%.2f%%' % (top_probability * 100)\n    #if confidence is low, set label as normal\n    if percentage < 0.30:\n        label=\"Normal\"\n    heatmap = explanations.explanations[0]['scores']\n    heatmap = cv2.resize(heatmap, (frame.shape[1], frame.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img  = cv2.addWeighted(heatmap, 0.3, frame, 0.8, 0)\n    #plt.imshow(frame+heatmap)\n    #print(superimposed_img.shape)\n    if label is \"Normal\":\n        color = (0, 150, 0)\n        frame = cv2.putText(frame, label+': '+top_probability, (50, 50), 2, 0.5 ,color, 1 )\n        videoWriter.write(frame)\n    else:\n        color = (0, 0, 150)\n        superimposed_img = cv2.putText(superimposed_img, label+': '+top_probability, (50, 50), 2, 0.5 ,color, 1 )\n        videoWriter.write(superimposed_img)\n\n\n\n    success, frame = videoCapture.read()\nvideoWriter.release()\n\nt2=time.time()\nprint('done', t2-t1, c)\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-02T14:29:21.590603Z","iopub.execute_input":"2022-12-02T14:29:21.590995Z","iopub.status.idle":"2022-12-02T14:29:32.463214Z","shell.execute_reply.started":"2022-12-02T14:29:21.590963Z","shell.execute_reply":"2022-12-02T14:29:32.462146Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"25.0\n","output_type":"stream"},{"name":"stderr","text":"OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n","output_type":"stream"},{"name":"stdout","text":"done 10.840048789978027 375\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}